{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers, Network and FUnction construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class noon:\n",
    "    def activate(self, input):\n",
    "        return input\n",
    "    def grad(self, input):\n",
    "        return 1\n",
    "class sigmoidActivation:\n",
    "    def activate(self, input):\n",
    "        return 1 / (1 + np.exp(-input))\n",
    "    def grad(self,input):\n",
    "        sig = self.activate(input)\n",
    "        return sig*(1-sig)\n",
    "\n",
    "\n",
    "class reluActivation:\n",
    "    def activate(self, input):\n",
    "        return np.maximum(0,input)\n",
    "    def grad(self,input):\n",
    "        input[input<=0] = 0\n",
    "        input[input>0] = 1\n",
    "        return input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss Functions\n",
    "class basicLoss:\n",
    "    def activate(self,y_t,y_pred):\n",
    "        y = y_pred-y_t\n",
    "        y=np.sum(y**2)\n",
    "        return y\n",
    "    def grad(self,y_t,y_pred):\n",
    "        y = y_pred-y_t\n",
    "        y = y*2\n",
    "        return y\n",
    "#regression\n",
    "def MSE(y_t, y_pred):\n",
    "    return np.mean(np.power(y_t-y_pred,2))\n",
    "#classification\n",
    "class crossEntropyLoss:\n",
    "    def activate(self,y_t, y_pred):\n",
    "        self.old_x = y_pred.clip(min=1e-8,max=None)\n",
    "        self.old_y = y_t\n",
    "        return np.where(y_pred==1,-np.log(self.old_x), 0)\n",
    "    def grad(self,y_t, y_pred):\n",
    "        out = np.where(self.old_y==1,-1/self.old_x, 0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network\\\n",
    "-Implemented by splitting components into classes, computationally inefficient but easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, numIn, numOut, activation=noon()):\n",
    "        '''\n",
    "            Initiate the Layer and variables to store relevant information.\n",
    "            Likely stores too much information and is not efficient.\n",
    "        '''\n",
    "        self.numIn = numIn\n",
    "        self.numOut = numOut\n",
    "        self.weights = np.random.rand(numIn,numOut)\n",
    "        self.biases = np.atleast_2d(np.zeros(numOut))\n",
    "        self.activation = activation\n",
    "\n",
    "        self.input = np.zeros(numIn)\n",
    "        self.weightedout = np.zeros(numOut)\n",
    "        self.out = np.zeros(numOut)\n",
    "\n",
    "        self.costGradW = np.random.rand(numIn,numOut)\n",
    "        self.costGradB = np.random.rand(numOut)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        weightedInputs = np.dot(inputs,self.weights)\n",
    "        out = weightedInputs + self.biases\n",
    "        self.weightedout = out\n",
    "        if self.activation:\n",
    "            out = self.activation.activate(out)\n",
    "        \n",
    "        self.out = out\n",
    "        self.input = inputs\n",
    "        return out\n",
    "\n",
    "    def applyGrad(self, lr):\n",
    "        \n",
    "        self.weights -= lr * self.costGradW\n",
    "        self.biases -= lr * self.costGradB\n",
    "\n",
    "    \n",
    "    def backward(self,grad):\n",
    "        '''\n",
    "            Backward calculations of a single layer, calculates\n",
    "            the gradient of the weights w.r.t the cost of the \n",
    "            network as a whole. Returns gradient of cost w.r.t\n",
    "            the input for the next layer.\n",
    "        '''\n",
    "        inp = np.atleast_2d(self.input)\n",
    "        grad = np.atleast_2d(grad)\n",
    "        \n",
    "        dact = np.atleast_2d(self.activation.grad(self.weightedout))\n",
    "        grad = dact*grad\n",
    "        \n",
    "        self.costGradB = np.atleast_2d(grad).mean(axis=0)\n",
    "        self.costGradW = np.matmul(inp.T,grad).mean(axis=0)\n",
    "\n",
    "        return np.dot(grad,self.weights.T)\n",
    "\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self,layers=[], lossFunction=basicLoss(), lr=0.001):\n",
    "        self.layers=layers\n",
    "        self.loss=None\n",
    "        self.lossFunction = lossFunction\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward_pass(self, inputs):\n",
    "        for Layer in self.layers:\n",
    "            inputs = Layer.forward(inputs)\n",
    "        \n",
    "        return np.atleast_2d(inputs)\n",
    "\n",
    "    def classify(self, inputs):\n",
    "        output = self.forward_pass(inputs)\n",
    "        return np.argmax(output)\n",
    "\n",
    "    def backprob(self,y_t,y_pred):\n",
    "        '''\n",
    "            This function takes the target and network predictions and performs \n",
    "            backpropagation by looping backward through the layers and calling \n",
    "            the backward function of the given layer. Grad is the running\n",
    "            gradient of the network required for the next layers calculations.\n",
    "        '''\n",
    "        grad = self.lossFunction.grad(y_t,y_pred)\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            grad = self.layers[i].backward(grad)\n",
    "        \n",
    "        for l in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[l].applyGrad(self.lr)\n",
    "\n",
    "    def compute_accuracy(self, x_val, y_val):\n",
    "        '''\n",
    "            This function does a forward pass of x, then checks if the indices\n",
    "            of the maximum value in the output equals the indices in the label\n",
    "            y. Then it sums over each prediction and calculates the accuracy.\n",
    "        '''\n",
    "        predictions = []\n",
    "\n",
    "        for x, y in zip(x_val, y_val):\n",
    "            output = self.forward_pass(x)\n",
    "            pred = np.argmax(output)\n",
    "            predictions.append(pred == np.argmax(y))\n",
    "        \n",
    "        out = np.mean(predictions)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def learn(self,input ,expected):\n",
    "        #change btw classify and forward_pass\n",
    "        out = self.forward_pass(input)\n",
    "        \n",
    "        loss = self.lossFunction.activate(expected,out)\n",
    "        self.backprob(expected,out)\n",
    "\n",
    "    def train(self,x_train,y_train,xval=None,yval=None,epochs=500, batchSize=1):\n",
    "        for iter in range(epochs):\n",
    "            for i in range(0,len(y_train)-1,batchSize):\n",
    "                \n",
    "                if i+batchSize < len(y_train):\n",
    "                    x = x_train[i:i+batchSize,:]\n",
    "                    y = y_train[i:i+batchSize,:]\n",
    "                else:\n",
    "                    x = x_train[i:-1,:]\n",
    "                    y = y_train[i:-1,:]\n",
    "                out = self.forward_pass(x)\n",
    "                \n",
    "                self.backprob(y,out)\n",
    "            if iter%10==0:\n",
    "                    out = self.forward_pass(x_train)\n",
    "                    print(self.compute_accuracy(x_train,y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=basicLoss()\n",
    "sig = sigmoidActivation()\n",
    "Layer1 = Layer(2,5)\n",
    "Layer2 = Layer(5,3,sig)\n",
    "Layero = Layer(3,2,sig)\n",
    "\n",
    "network = Network([Layer1,Layer2,Layero])\n",
    "\n",
    "testinput = np.array([2,2])\n",
    "\n",
    "#x = network.forward_pass(testinput)\n",
    "#x\n",
    "\n",
    "tx = [0.5,0.5]\n",
    "\n",
    "network.learn(testinput,tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived\n",
       "0           549\n",
       "1           342\n",
       "dtype: int64"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "sig = sigmoidActivation()\n",
    "relu = reluActivation()\n",
    "data = pd.read_csv('train.csv')\n",
    "data\n",
    "\n",
    "y_t = data[['Survived']]\n",
    "\n",
    "\n",
    "test_train = data[['Pclass','Sex']].copy()\n",
    "test_train['Sex'] = test_train['Sex'].map({'male':0, 'female':1})\n",
    "y_t.value_counts()\n",
    "\n",
    "#plt.scatter(data.Sex,data.Pclass,s=20,c=data.Survived,cmap='Greens')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = sigmoidActivation()\n",
    "relu = reluActivation()\n",
    "basic = basicLoss()\n",
    "\n",
    "Layer1 = Layer(2,3)\n",
    "Layero = Layer(3,1,sig)\n",
    "train = test_train.to_numpy()\n",
    "net = Network([Layer1,Layero],lossFunction=basic,lr=0.003)\n",
    "\n",
    "y = y_t.to_numpy()\n",
    "\n",
    "pre_score = 0\n",
    "for i in range(len(y)):\n",
    "    pred = net.forward_pass(train[i])\n",
    "    \n",
    "    pre_score+= np.sum(np.abs(pred-y[i]))\n",
    "    \n",
    "print(net.layers[0].weights)\n",
    "print(net.layers[1].weights)\n",
    "print(pre_score)\n",
    "\n",
    "for k in range(1000):\n",
    "    for j in range(len(y)):\n",
    "        net.learn(train[j],y[j])\n",
    "\n",
    "post_score = 0\n",
    "pred_sum=0\n",
    "for l in range(len(y)):\n",
    "    pred = net.forward_pass(train[l])\n",
    "    \n",
    "    pred_sum+=pred\n",
    "    post_score+= np.sum(np.abs(pred-y[l]))\n",
    "\n",
    "print(net.layers[0].weights)\n",
    "print(net.layers[1].weights)\n",
    "print(pred_sum)\n",
    "print(post_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3838383838383838\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.6161616161616161\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7867564534231201\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n",
      "0.7149270482603816\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=0)\n",
    "\n",
    "sig = sigmoidActivation()\n",
    "relu = reluActivation()\n",
    "basic = basicLoss()\n",
    "Layer1 = Layer(2,3)\n",
    "Layer2 = Layer(3,3)\n",
    "Layer3 = Layer(4,3)\n",
    "Layero = Layer(3,2,sig)\n",
    "\n",
    "net = Network([Layer1,Layer2,Layero],lossFunction=basic,lr=0.001)\n",
    "\n",
    "train = test_train.to_numpy()\n",
    "y = y_t.to_numpy()\n",
    "y2 = y.copy()\n",
    "y[y2==1]=0\n",
    "y[y2==0]=1\n",
    "expected = np.squeeze(np.array([y,y2])).T\n",
    "\n",
    "net.train(train,expected,epochs=500,batchSize=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()\n",
    "y2 = y.copy()\n",
    "y[y2==1]=0\n",
    "y[y2==0]=1\n",
    "expected = np.squeeze(np.array([y,y2])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.5\n",
      "0.53\n",
      "0.56\n",
      "0.58\n",
      "0.61\n",
      "0.62\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.64\n",
      "0.63\n",
      "0.62\n",
      "0.6\n",
      "0.58\n",
      "0.61\n",
      "0.61\n",
      "0.61\n",
      "0.61\n",
      "0.65\n",
      "0.65\n",
      "0.68\n",
      "0.7\n",
      "0.72\n",
      "0.71\n",
      "0.71\n",
      "0.72\n",
      "0.71\n",
      "0.71\n",
      "0.69\n",
      "0.68\n",
      "0.67\n",
      "0.66\n",
      "0.64\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.65\n",
      "0.64\n",
      "0.64\n",
      "0.65\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.66\n",
      "0.67\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.67\n",
      "0.67\n",
      "0.67\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.68\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.69\n",
      "0.7\n",
      "0.7\n",
      "0.7\n",
      "0.7\n",
      "0.7\n",
      "0.7\n",
      "0.7\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "sig = sigmoidActivation()\n",
    "relu = reluActivation()\n",
    "basic = basicLoss()\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "Layer1 = Layer(2,3)\n",
    "Layer2=Layer(3,3,sig)\n",
    "Layer3=Layer(3,3,sig)\n",
    "Layero = Layer(3,2,sig)\n",
    "\n",
    "net = Network([Layer1,Layer2,Layer3,Layero],lossFunction=basic,lr=0.05)\n",
    "\n",
    "net.train(X,expected,epochs=2000,batchSize=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
